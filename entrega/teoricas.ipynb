{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importância de se utilizar o Fake LLM para testes rápidos advém da possibilidade de poder realizar testes do aplicativo como um todo, incluindo da arquitetura, sem precisar consumir recursos. Ou seja, é possível testar a aplicação sem a necessidade de gastar chamadas, que gerariam gastos financeiros, ou recursos computacionais como, por exemplo, GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama\n",
    "\n",
    "![Diagrama](../images/Q1-Diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrama\n",
    "\n",
    "![Diagram](../images/Q2-Diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrama\n",
    "\n",
    "![Diagrama2](../images/Q3-Diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos o modelo do Gemini ao invés do Open AI, assim faremos a análise para o Gemini.\n",
    "\n",
    "Com relação as limitações, foi possível observar uma maior latência na resposta quando comparado com a execução local de LLMs uma vez que tem que ser realizada uma requisição até a API do Google para se obter uma resposta. Sobre esse ponto, observamos também que o tempo de latência para a execução uma resposta do 1.5 Flash tornou a aplicação inviável, justificando assim a escola pelo modelo 1.5 Pro. Além disso, a latência pode ainda ser influenciada por outros fatores como a complexidade da tarefa e do prompt a serem processados pelo LLM. \n",
    "\n",
    "Ainda temos a questão dos limites de uso das APIs, bem como seus custos que podem impactar a escalabilidade de uma aplicação que utilize essa ferramenta, pois à medida que o volume de solicitações aumenta, os custos associados ao uso da API também aumentam. Atualmente as limitações e custos dos modelos do Google Gemini, inclusive o utilizado neste TP, o Gemini 1.5 Pro, podem ser consultados em [Pricing Models](https://ai.google.dev/pricing).\n",
    "\n",
    "Por fim, a qualidade da tradução do Gemini 1.5 Pro foi a mesma demonstrada tanto pelos modelos on premisses quanto pelo Gemini 1.5 Flash. Todavia, a de se considerar que os testes foram realizados apenas com prompts simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O uso do langchain adiciona uma camada extra a aplicação, o que pode levar a uma maior latência e consumo de recursos computacionais, gerando impacto em aplicações que exijam respostas em tempo real, todavia não observamos esse fenômeno na nossa aplicação, sendo o comportamento muito paracido com a aplicação desenvolvida na parte 1.\n",
    "\n",
    "Todavia, quando em comparação ao uso direto da API do Hugging Face, o uso do langchain melhora o desempenho dos resultados apresentados pelo modelo uma vez que permite melhores adaptações e fornece recursos extras ao desenvolvedor. Ainda, o lagchain possui diversos recursos modulares que garantem maior flexibilidade no desenvolvimento como por exemplo:\n",
    "\n",
    "1. Melhoria de Desempenho: Estratégias de ação e memória que aumentam a confiabilidade e reduzem erros nas interações.\n",
    "\n",
    "2. Redução de Complexidade: Abstrações que simplificam a engenharia de prompts e a integração de dados, reduzindo a curva de aprendizado.\n",
    "\n",
    "3. Encadeamento de Serviços: Suporte para integração de múltiplos serviços além de LLMs, ampliando as capacidades das aplicações.\n",
    "\n",
    "4. Interações de Agentes Guiadas por Objetivos: Substituição de chamadas isoladas por interações contínuas e orientadas a metas, otimizando a eficácia das respostas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama](../images/Q6-Diagrama.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
