{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importância de se utilizar o Fake LLM para testes rápidos advém da possibilidade de poder realizar testes do aplicativo como um todo, incluindo da arquitetura, sem precisar consumir recursos. Ou seja, é possível testar a aplicação sem a necessidade de gastar chamadas, que gerariam gastos financeiros, ou recursos computacionais como, por exemplo, GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama\n",
    "\n",
    "![Diagrama](../images/Q1-Diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrama\n",
    "\n",
    "![Diagram](../images/Q2-Diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrama\n",
    "\n",
    "![Diagrama2](../images/Q3-Diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos o modelo do Gemini ao invés do Open AI, assim faremos a análise para o Gemini.\n",
    "\n",
    "Com relação as limitações, foi possível observar uma maior latência na resposta quando comparado com a execução local de LLMs uma vez que tem que ser realizada uma requisição até a API do Google para se obter uma resposta. Sobre esse ponto, observamos também que o tempo de latência para a execução uma resposta do 1.5 Flash tornou a aplicação inviável, justificando assim a escola pelo modelo 1.5 Pro. Além disso, a latência pode ainda ser influenciada por outros fatores como a complexidade da tarefa e do prompt a serem processados pelo LLM. \n",
    "\n",
    "Ainda temos a questão dos limites de uso das APIs, bem como seus custos que podem impactar a escalabilidade de uma aplicação que utilize essa ferramenta, pois à medida que o volume de solicitações aumenta, os custos associados ao uso da API também aumentam. Atualmente as limitações e custos dos modelos do Google Gemini, inclusive o utilizado neste TP, o Gemini 1.5 Pro, podem ser consultados em [Pricing Models](https://ai.google.dev/pricing).\n",
    "\n",
    "Por fim, a qualidade da tradução do Gemini 1.5 Pro foi a mesma demonstrada tanto pelos modelos on premisses quanto pelo Gemini 1.5 Flash. Todavia, a de se considerar que os testes foram realizados apenas com prompts simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
